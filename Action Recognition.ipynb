{"cells":[{"cell_type":"code","source":["# Python way (do this early in the notebook)\n","import os\n","os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\"\n"],"metadata":{"id":"5-P9oM60zHty","executionInfo":{"status":"ok","timestamp":1761748016910,"user_tz":-570,"elapsed":37,"user":{"displayName":"Avery Doan (AveryD)","userId":"04901609193817367419"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# Colab magic (affects the whole process)\n","%env PYDEVD_DISABLE_FILE_VALIDATION=1\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RKG--aC3zKfH","executionInfo":{"status":"ok","timestamp":1761748017896,"user_tz":-570,"elapsed":6,"user":{"displayName":"Avery Doan (AveryD)","userId":"04901609193817367419"}},"outputId":"7b080051-e465-494c-e7ea-bc7dea32931d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["env: PYDEVD_DISABLE_FILE_VALIDATION=1\n"]}]},{"cell_type":"code","source":["# All of these are NOT required for your Gradio + MediaPipe + TF app.\n","# They pull conflicting pins (NumPy 2.x, protobuf 5.x, headless OpenCV, etc.).\n","!pip -q uninstall -y \\\n","  albumentations albucore \\\n","  tensorflow-hub tf-keras tensorflow-text \\\n","  grpcio-status opentelemetry-proto \\\n","  pytensor thinc spacy \\\n","  flax jax jaxlib jax_cuda12_plugin \\\n","  orbax-checkpoint tensorstore ml-dtypes \\\n","  tensorflow-decision-forests ydf \\\n","  dopamine-rl keras-hub \\\n","  opencv-python-headless || true\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TLgvpx1IyIjR","executionInfo":{"status":"ok","timestamp":1761748678298,"user_tz":-570,"elapsed":10297,"user":{"displayName":"Avery Doan (AveryD)","userId":"04901609193817367419"}},"outputId":"2d87ced1-2a69-43b0-bbb5-dcbf5ddc7424"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping tensorflow-text as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Skipping flax as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Skipping jax_cuda12_plugin as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Skipping orbax-checkpoint as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Skipping tensorstore as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Skipping tensorflow-decision-forests as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Skipping ydf as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Skipping dopamine-rl as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Skipping keras-hub as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Skipping opencv-python-headless as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip -q install --upgrade \\\n","  \"numpy==1.26.4\" \\\n","  \"protobuf==4.25.3\" \\\n","  \"tensorflow==2.17.1\" \\\n","  \"opencv-python==4.10.0.84\" \\\n","  \"mediapipe==0.10.14\" \\\n","  \"scikit-learn\" \\\n","  \"matplotlib\" \\\n","  \"tqdm\" \\\n","  \"gradio\"\n"],"metadata":{"id":"NbWEVCbCUlwA","executionInfo":{"status":"ok","timestamp":1761748778882,"user_tz":-570,"elapsed":13228,"user":{"displayName":"Avery Doan (AveryD)","userId":"04901609193817367419"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf, mediapipe as mp, cv2, numpy as np, gradio as gr, sklearn\n","print(\"TF:\", tf.__version__)          # expect 2.17.1 (or your chosen TF)\n","print(\"MP:\", mp.__version__)          # 0.10.14 if you followed prior pins\n","print(\"NP:\", np.__version__)          # 1.26.4\n","print(\"cv2:\", cv2.__version__)        # 4.10.0.84\n","print(\"gradio:\", gr.__version__)      # current\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g2ruEtyFWLnN","executionInfo":{"status":"ok","timestamp":1761748801015,"user_tz":-570,"elapsed":19267,"user":{"displayName":"Avery Doan (AveryD)","userId":"04901609193817367419"}},"outputId":"07203b7e-79e9-465c-c517-cd5228598856"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["TF: 2.17.1\n","MP: 0.10.14\n","NP: 1.26.4\n","cv2: 4.11.0\n","gradio: 5.49.1\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"108Jz_Giuuut","executionInfo":{"status":"ok","timestamp":1761748869370,"user_tz":-570,"elapsed":46,"user":{"displayName":"Avery Doan (AveryD)","userId":"04901609193817367419"}}},"outputs":[],"source":["import os, cv2, math, json, shutil, glob, io, time\n","import numpy as np\n","import gradio as gr\n","import mediapipe as mp\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix\n","import tensorflow as tf\n","from tensorflow.keras import layers, models"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eRl_e9SCwQvd","executionInfo":{"status":"ok","timestamp":1761748895502,"user_tz":-570,"elapsed":22111,"user":{"displayName":"Avery Doan (AveryD)","userId":"04901609193817367419"}},"outputId":"c7b2756b-a994-4be8-8523-39f1c40998fb"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"id":"GcPLR-rbu0Pg","executionInfo":{"status":"ok","timestamp":1761748926617,"user_tz":-570,"elapsed":1193,"user":{"displayName":"Avery Doan (AveryD)","userId":"04901609193817367419"}}},"outputs":[],"source":["USE_DRIVE = False  # << set True if you mounted Drive\n","BASE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Action Recognition\"\n","DATA_PATH = os.path.join(BASE_DIR, \"MP_Data\")\n","MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n","os.makedirs(DATA_PATH, exist_ok=True)\n","os.makedirs(MODELS_DIR, exist_ok=True)"]},{"cell_type":"markdown","source":["## First version"],"metadata":{"id":"R2ILL4VgmrBB"}},{"cell_type":"code","source":["# @title\n","# === Gradio Action Detection App (fixed, image-safe) ===\n","# Requires (install first in Colab):\n","# !pip -q install gradio mediapipe opencv-python tensorflow scikit-learn matplotlib tqdm pillow\n","\n","import os, io, glob, json, time, shutil\n","from pathlib import Path\n","import numpy as np\n","import cv2\n","import gradio as gr\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix\n","import tensorflow as tf\n","from tensorflow.keras import layers, models\n","\n","# ---- Config / Paths ----\n","USE_DRIVE = False\n","BASE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Action Recognition\"  # your path\n","DATA_PATH = os.path.join(BASE_DIR, \"MP_Data\")\n","MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n","os.makedirs(DATA_PATH, exist_ok=True)\n","os.makedirs(MODELS_DIR, exist_ok=True)\n","\n","# ---- MediaPipe for keypoints ----\n","import mediapipe as mp\n","mp_holistic = mp.solutions.holistic\n","\n","def extract_keypoints(results):\n","    def arr(vals, pad):\n","        a = np.array(vals).flatten()\n","        if a.size == 0:\n","            return np.zeros(pad)\n","        if a.size < pad:\n","            a = np.pad(a, (0, pad - a.size))\n","        return a\n","    pose = arr([[r.x, r.y, r.z, r.visibility] for r in (results.pose_landmarks.landmark if results.pose_landmarks else [])], 132)\n","    face = arr([[r.x, r.y, r.z] for r in (results.face_landmarks.landmark if results.face_landmarks else [])], 1404)\n","    lh   = arr([[r.x, r.y, r.z] for r in (results.left_hand_landmarks.landmark if results.left_hand_landmarks else [])], 63)\n","    rh   = arr([[r.x, r.y, r.z] for r in (results.right_hand_landmarks.landmark if results.right_hand_landmarks else [])], 63)\n","    return np.concatenate([pose, face, lh, rh])  # (1662,)\n","\n","def video_to_sequence_keypoints(video_path, sequence_length=30, stride=1):\n","    cap = cv2.VideoCapture(video_path)\n","    frames = []\n","    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n","        i = 0\n","        while True:\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            if i % max(1, int(stride)) != 0:\n","                i += 1\n","                continue\n","            img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            img.flags.writeable = False\n","            results = holistic.process(img)\n","            img.flags.writeable = True\n","            frames.append(extract_keypoints(results))\n","            if len(frames) >= sequence_length:\n","                break\n","            i += 1\n","    cap.release()\n","    if len(frames) == 0:\n","        frames = [np.zeros(1662) for _ in range(sequence_length)]\n","    elif len(frames) < sequence_length:\n","        last = frames[-1]\n","        frames += [last] * (sequence_length - len(frames))\n","    else:\n","        frames = frames[:sequence_length]\n","    return np.stack(frames, axis=0)  # (T,1662)\n","\n","# ---- Image helpers (return NumPy arrays for gr.Image) ----\n","def fig_to_array(fig):\n","    buf = io.BytesIO()\n","    fig.savefig(buf, format=\"png\", bbox_inches=\"tight\")\n","    buf.seek(0)\n","    arr = np.array(Image.open(buf).convert(\"RGB\"))\n","    buf.close()\n","    return arr\n","\n","def plot_confmat(cm, labels):\n","    fig = plt.figure(figsize=(4 + 0.3 * len(labels), 4 + 0.3 * len(labels)))\n","    plt.imshow(cm, interpolation='nearest')\n","    plt.title(\"Confusion Matrix\")\n","    plt.colorbar()\n","    ticks = np.arange(len(labels))\n","    plt.xticks(ticks, labels, rotation=45, ha=\"right\")\n","    plt.yticks(ticks, labels)\n","    thresh = cm.max() / 2\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            plt.text(j, i, int(cm[i, j]), ha=\"center\", va=\"center\",\n","                     color=\"white\" if cm[i, j] > thresh else \"black\")\n","    plt.xlabel(\"Predicted\")\n","    plt.ylabel(\"True\")\n","    plt.tight_layout()\n","    arr = fig_to_array(fig)\n","    plt.close(fig)\n","    return arr  # NumPy array for gr.Image\n","\n","# ---- Dataset & Model helpers ----\n","def list_actions():\n","    return sorted([d for d in os.listdir(DATA_PATH) if os.path.isdir(os.path.join(DATA_PATH, d))])\n","\n","def count_sequences(action):\n","    return len([p for p in glob.glob(os.path.join(DATA_PATH, action, \"*\")) if os.path.isdir(p)])\n","\n","def save_sequence(action, sequence_array):\n","    act_dir = os.path.join(DATA_PATH, action)\n","    os.makedirs(act_dir, exist_ok=True)\n","    next_id = count_sequences(action) + 1\n","    seq_dir = os.path.join(act_dir, str(next_id))\n","    os.makedirs(seq_dir, exist_ok=True)\n","    for i, f in enumerate(sequence_array):\n","        np.save(os.path.join(seq_dir, f\"{i}.npy\"), f)\n","    return next_id\n","\n","def load_dataset(actions=None, sequence_length=30):\n","    if not actions:\n","        actions = list_actions()\n","    a2i = {a: i for i, a in enumerate(actions)}\n","    X, y = [], []\n","    for a in actions:\n","        seq_dirs = sorted(\n","            [d for d in glob.glob(os.path.join(DATA_PATH, a, \"*\")) if os.path.isdir(d)],\n","            key=lambda p: int(os.path.basename(p)) if os.path.basename(p).isdigit() else 0\n","        )\n","        for sd in seq_dirs:\n","            frames = []\n","            for i in range(int(sequence_length)):\n","                fpath = os.path.join(sd, f\"{i}.npy\")\n","                frames.append(np.load(fpath) if os.path.exists(fpath) else np.zeros(1662))\n","            X.append(np.stack(frames))\n","            y.append(a2i[a])\n","    if len(X) == 0:\n","        return None, None, actions\n","    return np.array(X, dtype=np.float32), np.array(y, dtype=np.int64), actions\n","\n","def build_lstm_model(num_classes, sequence_length=30, feature_dim=1662,\n","                     lstm_units=128, dense_units=64, dropout=0.3, force_cpu=False):\n","    \"\"\"\n","    No Masking layer (fixed-length sequences); force non-cuDNN LSTM so masks are irrelevant.\n","    \"\"\"\n","    # Using activation / implementation settings that disable cuDNN fast path.\n","    lstm_kwargs = dict(\n","        return_sequences=True,\n","        activation=\"tanh\",\n","        recurrent_activation=\"sigmoid\",\n","        implementation=2,        # use standard kernel\n","        recurrent_dropout=0.0,\n","        dropout=0.0,\n","    )\n","\n","    inputs = layers.Input(shape=(sequence_length, feature_dim))\n","    x = layers.LSTM(lstm_units, **lstm_kwargs)(inputs)\n","    x = layers.Dropout(dropout)(x)\n","    x = layers.LSTM(lstm_units, activation=\"tanh\", recurrent_activation=\"sigmoid\",\n","                    implementation=2, recurrent_dropout=0.0, dropout=0.0)(x)\n","    x = layers.Dropout(dropout)(x)\n","    x = layers.Dense(dense_units, activation=\"relu\")(x)\n","    x = layers.Dropout(dropout)(x)\n","    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n","    model = models.Model(inputs, outputs)\n","    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","    return model\n","\n","\n","\n","# ---- Tab 1 handlers (Collect) ----\n","def create_action_folder(action_name_text, target_count):\n","    if not action_name_text or str(action_name_text).strip() == \"\":\n","        return \"Enter an action name.\", gr.update(value=\"\"), \"0 / 0\"\n","    p = os.path.join(DATA_PATH, action_name_text)\n","    os.makedirs(p, exist_ok=True)\n","    current = count_sequences(action_name_text)\n","    total = int(target_count)\n","    return f\"Using folder: {p}\", gr.update(value=action_name_text), f\"{current} / {total}\"\n","\n","def record_and_save(action_name_text, video, sequence_length, stride, target_count):\n","    if not action_name_text or str(action_name_text).strip() == \"\":\n","        return \"Pick/create an action first.\", None\n","    if video is None:\n","        return \"Record or upload a short clip.\", None\n","    seq = video_to_sequence_keypoints(video, sequence_length=int(sequence_length), stride=max(1, int(stride)))\n","    _ = save_sequence(action_name_text, seq)\n","    progress = f\"{count_sequences(action_name_text)} / {int(target_count)}\"\n","    return f\"Saved 1 sequence in '{action_name_text}'.\", progress\n","\n","def reset_action(action_name_text, really=False):\n","    if not action_name_text or str(action_name_text).strip() == \"\":\n","        return \"Enter an action name.\", gr.update(value=\"\")\n","    if not really:\n","        return \"Tick the checkbox to confirm reset.\", gr.update(value=action_name_text)\n","    p = os.path.join(DATA_PATH, action_name_text)\n","    if os.path.exists(p):\n","        shutil.rmtree(p)\n","    os.makedirs(p, exist_ok=True)\n","    return f\"Cleared data for '{action_name_text}'.\", gr.update(value=action_name_text)\n","\n","# ---- Tab 2 handler (Train/Evaluate) ----\n","def train_model(sequence_length, test_size, epochs, batch_size):\n","    X, y, actions = load_dataset(sequence_length=int(sequence_length))\n","    if X is None:\n","        return \"Dataset empty. Collect sequences first.\", None, None, None\n","\n","    # --- Sanitize data: dtype, NaNs/Infs, optional normalization ---\n","    X = np.asarray(X, dtype=np.float32)\n","    y = np.asarray(y, dtype=np.int32)\n","    # Replace NaN/Inf with 0\n","    X[~np.isfinite(X)] = 0.0\n","\n","    # Optional: simple per-feature standardization over the training set\n","    # (do it before splitting to avoid tiny-set blowups; this is a small demo)\n","    mean = X.mean(axis=(0, 1), keepdims=True)\n","    std = X.std(axis=(0, 1), keepdims=True) + 1e-6\n","    X = (X - mean) / std\n","\n","    # --- Decide split strategy (avoid failing on tiny/imbalanced sets) ---\n","    counts = np.bincount(y, minlength=len(actions))\n","    can_strat = (len(X) > len(actions)) and np.all(counts >= 2)\n","\n","    if can_strat:\n","        tsize = min(float(test_size), 0.2)\n","        Xtr, Xte, ytr, yte = train_test_split(\n","            X, y, test_size=tsize, random_state=42, stratify=y\n","        )\n","        val_data = (Xte, yte)\n","        msg = f\"Stratified split. Class counts: {dict(zip(actions, counts))}\"\n","    else:\n","        Xtr, ytr = X, y\n","        val_data = None\n","        msg = f\"No validation split (class counts: {dict(zip(actions, counts))}).\"\n","\n","    bs = max(1, min(int(batch_size), len(Xtr)))\n","\n","    # --- Build & train (no cuDNN path) ---\n","    model = build_lstm_model(num_classes=len(actions), sequence_length=int(sequence_length))\n","    history = model.fit(\n","        Xtr, ytr, validation_data=val_data, epochs=int(epochs), batch_size=bs, verbose=0\n","    )\n","\n","    # --- Save artifacts ---\n","    ts = int(time.time())\n","    model_path = os.path.join(MODELS_DIR, f\"lstm_{ts}.keras\")\n","    labels_path = os.path.join(MODELS_DIR, f\"lstm_{ts}.labels.json\")\n","    model.save(model_path)\n","    with open(labels_path, \"w\") as f:\n","        json.dump(actions, f)\n","\n","    # --- Loss plot -> numpy array for gr.Image ---\n","    fig = plt.figure(figsize=(8, 3))\n","    plt.plot(history.history.get(\"loss\", []), label=\"train\")\n","    if \"val_loss\" in history.history:\n","        plt.plot(history.history[\"val_loss\"], label=\"val\")\n","    plt.title(\"Loss\"); plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.legend()\n","    loss_arr = fig_to_array(fig); plt.close(fig)\n","\n","    # --- Validation metrics if we had a val set ---\n","    if val_data is not None and len(val_data[0]) > 0:\n","        Xte, yte = val_data\n","        ypred = np.argmax(model.predict(Xte, verbose=0), axis=1)\n","        cm_arr = plot_confmat(confusion_matrix(yte, ypred), actions)\n","        report = classification_report(yte, ypred, target_names=actions)\n","    else:\n","        cm_arr = None\n","        report = (\"No validation metrics (insufficient sequences per class). \"\n","                  \"Collect ≥2 per action (10+ preferred).\")\n","\n","    summary = f\"{msg}\\nTrained on {len(Xtr)} sequences (batch_size={bs}).\\nSaved model:\\n{model_path}\\n{labels_path}\"\n","    return summary, loss_arr, cm_arr, report\n","\n","    # after model/labels are saved in train_model()\n","    norm_path = os.path.join(MODELS_DIR, f\"lstm_{ts}.norm.npz\")\n","    np.savez_compressed(norm_path,\n","                        mean=mean.astype(np.float32),\n","                        std=std.astype(np.float32),\n","                        sequence_length=int(sequence_length))\n","    summary = (f\"{msg}\\nTrained on {len(Xtr)} sequences (batch_size={bs}).\\n\"\n","              f\"Saved model:\\n{model_path}\\n{labels_path}\\n{norm_path}\")\n","\n","\n","\n","# ---- Tab 3 handlers (Test) ----\n","def _load_model_bundle(model_path, labels_path):\n","    model = tf.keras.models.load_model(model_path, compile=False)\n","    with open(labels_path, \"r\") as f:\n","        actions = json.load(f)\n","    # try to find matching normalization file\n","    base = os.path.splitext(os.path.basename(model_path))[0]   # e.g., \"lstm_173023...\"\n","    norm_guess = os.path.join(os.path.dirname(model_path), f\"{base}.norm.npz\")\n","    norm = None\n","    if os.path.exists(norm_guess):\n","        norm = np.load(norm_guess)\n","    return model, actions, norm\n","\n","def _apply_norm(seq, norm):\n","    if norm is None:\n","        return seq\n","    mean = norm[\"mean\"]  # shape (1,1,1662)\n","    std  = norm[\"std\"]   # shape (1,1,1662)\n","    return (seq - mean) / (std + 1e-6)\n","\n","def _nonzero_landmark_ratio(seq):\n","    # seq shape: (T, 1662). Count frames with any nonzero landmark.\n","    nonzero = (np.abs(seq).sum(axis=1) > 1e-8).astype(np.float32)\n","    return float(nonzero.mean()), int(nonzero.sum()), int(seq.shape[0])\n","\n","def _predict_seq(model, actions, seq, conf_floor=0.40, topk=5):\n","    logits = model.predict(np.expand_dims(seq, 0), verbose=0)[0]\n","    idx = np.argsort(logits)[::-1]\n","    idx = idx[:min(topk, len(actions))]\n","    top = [(actions[i], float(logits[i])) for i in idx]\n","    max_lab, max_prob = top[0]\n","    status = \"ok\" if max_prob >= conf_floor else \"low_confidence\"\n","    return top, status\n","\n","def predict_from_webcam(model_path, labels_path, video, sequence_length, stride):\n","    if not model_path or not labels_path:\n","        return \"Provide model (.keras) and labels (.json).\", None\n","    if video is None:\n","        return \"Record a short clip.\", None\n","\n","    model, actions, norm = _load_model_bundle(model_path, labels_path)\n","\n","    # Basic sanity: model output vs labels\n","    out_dim = model.output_shape[-1]\n","    if out_dim != len(actions):\n","        return (f\"Label/model mismatch: model has {out_dim} outputs but {len(actions)} labels. \"\n","                \"Make sure the .labels.json matches this .keras.\"), None\n","\n","    # Build sequence\n","    seq = video_to_sequence_keypoints(video, sequence_length=int(sequence_length),\n","                                      stride=max(1, int(stride)))\n","\n","    # If we saved a different training sequence length, adapt gracefully\n","    if norm is not None and \"sequence_length\" in norm.files:\n","        train_T = int(norm[\"sequence_length\"])\n","        if seq.shape[0] != train_T:\n","            # center-crop or pad with last frame to match train length\n","            if seq.shape[0] > train_T:\n","                start = max(0, (seq.shape[0]-train_T)//2)\n","                seq = seq[start:start+train_T]\n","            else:\n","                seq = np.concatenate([seq, np.repeat(seq[-1:],[train_T-seq.shape[0]], axis=0)], axis=0)\n","\n","    # Apply same normalization\n","    seq = _apply_norm(seq.astype(np.float32), norm)\n","\n","    # Diagnostics: landmark presence\n","    nz_ratio, nz_frames, total_frames = _nonzero_landmark_ratio(seq)\n","    diag = {\"nonzero_ratio\": round(nz_ratio, 3),\n","            \"nonzero_frames\": nz_frames,\n","            \"total_frames\": total_frames}\n","\n","    # Predict\n","    top, status = _predict_seq(model, actions, seq, conf_floor=0.40, topk=min(5, len(actions)))\n","    diag[\"status\"] = status\n","    diag[\"top1\"] = {\"label\": top[0][0], \"prob\": round(top[0][1], 3)}\n","\n","    # Plot bars\n","    fig = plt.figure(figsize=(6, 3))\n","    labs = [t[0] for t in top]\n","    scores = [t[1] for t in top]\n","    plt.bar(labs, scores)\n","    plt.ylim(0, 1.0)\n","    plt.title(\"Top Predictions\")\n","    pred_arr = fig_to_array(fig); plt.close(fig)\n","\n","    return json.dumps({\"preds\": top, \"diag\": diag}, indent=2), pred_arr\n","\n","\n","def predict_from_dataset(model_path, labels_path, action_name_drop, seq_index, sequence_length):\n","    if not model_path or not labels_path:\n","        return \"Provide model (.keras) and labels (.json).\", None\n","    if not action_name_drop:\n","        return \"Select an action.\", None\n","\n","    seq_dir = os.path.join(DATA_PATH, action_name_drop, str(int(seq_index)))\n","    if not os.path.isdir(seq_dir):\n","        return f\"No sequence #{int(seq_index)} for '{action_name_drop}'.\", None\n","\n","    model, actions, norm = _load_model_bundle(model_path, labels_path)\n","\n","    out_dim = model.output_shape[-1]\n","    if out_dim != len(actions):\n","        return (f\"Label/model mismatch: model has {out_dim} outputs but {len(actions)} labels. \"\n","                \"Make sure the .labels.json matches this .keras.\"), None\n","\n","    # Load saved frames\n","    frames = []\n","    for i in range(int(sequence_length)):\n","        fpath = os.path.join(seq_dir, f\"{i}.npy\")\n","        frames.append(np.load(fpath) if os.path.exists(fpath) else np.zeros(1662))\n","    seq = np.stack(frames).astype(np.float32)\n","\n","    # Align to train sequence length if needed\n","    if norm is not None and \"sequence_length\" in norm.files:\n","        train_T = int(norm[\"sequence_length\"])\n","        if seq.shape[0] != train_T:\n","            if seq.shape[0] > train_T:\n","                start = max(0, (seq.shape[0]-train_T)//2)\n","                seq = seq[start:start+train_T]\n","            else:\n","                seq = np.concatenate([seq, np.repeat(seq[-1:],[train_T-seq.shape[0]], axis=0)], axis=0)\n","\n","    seq = _apply_norm(seq, norm)\n","\n","    nz_ratio, nz_frames, total_frames = _nonzero_landmark_ratio(seq)\n","    diag = {\"nonzero_ratio\": round(nz_ratio, 3),\n","            \"nonzero_frames\": nz_frames,\n","            \"total_frames\": total_frames}\n","\n","    top, status = _predict_seq(model, actions, seq, conf_floor=0.40, topk=min(5, len(actions)))\n","    diag[\"status\"] = status\n","    diag[\"top1\"] = {\"label\": top[0][0], \"prob\": round(top[0][1], 3)}\n","\n","    fig = plt.figure(figsize=(6, 3))\n","    labs = [t[0] for t in top]\n","    scores = [t[1] for t in top]\n","    plt.bar(labs, scores)\n","    plt.ylim(0, 1.0)\n","    plt.title(\"Top Predictions (Dataset)\")\n","    pred_arr = fig_to_array(fig); plt.close(fig)\n","    return json.dumps({\"preds\": top, \"diag\": diag}, indent=2), pred_arr\n","\n","\n","# ==================== Gradio UI ====================\n","with gr.Blocks(title=\"Action Detection — Collect / Train / Test\") as demo:\n","    gr.Markdown(f\"**Base directory:** `{BASE_DIR}`  \\nKeypoints → `{DATA_PATH}`  \\nModels → `{MODELS_DIR}`\")\n","\n","    # Tab 1: Collect\n","    with gr.Tab(\"1) Collect\"):\n","        with gr.Row():\n","            action_name_tb = gr.Textbox(label=\"Action name\", placeholder=\"e.g., wave, hello, thanks\")\n","            target_count = gr.Number(label=\"Target sequences\", value=30, precision=0)\n","            create_btn = gr.Button(\"Create / Select Folder\")\n","        status1 = gr.Markdown(\"\")\n","        progress = gr.Textbox(label=\"Progress\", value=\"0 / 30\", interactive=False)\n","\n","        gr.Markdown(\"**Record a short clip (5–10s) for ONE sequence**. Repeat until you reach your target.\")\n","        with gr.Row():\n","            sequence_length = gr.Slider(10, 60, value=30, step=1, label=\"Frames per sequence\")\n","            stride = gr.Slider(1, 5, value=1, step=1, label=\"Stride (use every Nth frame)\")\n","        video_in = gr.Video(label=\"Video (webcam or upload)\", sources=[\"webcam\", \"upload\"], height=300)\n","        save_btn = gr.Button(\"Record & Save (one sequence)\")\n","\n","        really = gr.Checkbox(label=\"Really reset this action's data?\")\n","        reset_btn = gr.Button(\"Reset Action Folder\", variant=\"stop\")\n","\n","        create_btn.click(create_action_folder, [action_name_tb, target_count], [status1, action_name_tb, progress])\n","        save_btn.click(record_and_save, [action_name_tb, video_in, sequence_length, stride, target_count], [status1, progress])\n","        reset_btn.click(reset_action, [action_name_tb, really], [status1, action_name_tb])\n","\n","    # Tab 2: Train / Evaluate\n","    with gr.Tab(\"2) Train / Evaluate\"):\n","        with gr.Row():\n","            seq_len_train = gr.Slider(10, 60, value=30, step=1, label=\"Sequence length\")\n","            test_size = gr.Slider(0.1, 0.5, value=0.2, step=0.05, label=\"Test size\")\n","        with gr.Row():\n","            epochs = gr.Slider(1, 50, value=12, step=1, label=\"Epochs\")\n","            batch = gr.Slider(4, 64, value=16, step=4, label=\"Batch size\")\n","        train_btn = gr.Button(\"Train LSTM\")\n","        train_msg = gr.Textbox(label=\"Summary\", lines=4)\n","        loss_img = gr.Image(label=\"Training Loss\")\n","        cm_img = gr.Image(label=\"Confusion Matrix\")\n","        report_txt = gr.Textbox(label=\"Classification Report\", lines=14)\n","        train_btn.click(train_model, [seq_len_train, test_size, epochs, batch], [train_msg, loss_img, cm_img, report_txt])\n","\n","    # Tab 3: Test\n","    with gr.Tab(\"3) Test\"):\n","        gr.Markdown(\"Test with **webcam** or an existing **dataset sequence**.\")\n","        with gr.Row():\n","            with gr.Column():\n","                gr.Markdown(\"**Webcam Test**\")\n","                model_path = gr.Textbox(label=\"Model .keras path\")\n","                labels_path = gr.Textbox(label=\"Labels .json path\")\n","                seq_len_inf = gr.Slider(10, 60, value=30, step=1, label=\"Sequence length\")\n","                stride_inf = gr.Slider(1, 5, value=1, step=1, label=\"Stride\")\n","                vid_inf = gr.Video(label=\"Video (webcam or upload)\", sources=[\"webcam\", \"upload\"], height=300)\n","                infer_btn = gr.Button(\"Predict (Webcam/Upload)\")\n","                preds_json = gr.Textbox(label=\"Top-k predictions (JSON)\", lines=8)\n","                preds_plot = gr.Image(label=\"Scores\")\n","                infer_btn.click(predict_from_webcam, [model_path, labels_path, vid_inf, seq_len_inf, stride_inf], [preds_json, preds_plot])\n","\n","            with gr.Column():\n","                gr.Markdown(\"**Dataset Test**\")\n","                action_sel = gr.Dropdown(label=\"Action\", choices=list_actions())\n","                seq_idx = gr.Number(label=\"Sequence #\", value=1, precision=0)\n","                seq_len_ds = gr.Slider(10, 60, value=30, step=1, label=\"Sequence length\")\n","                infer_ds_btn = gr.Button(\"Predict (Dataset)\")\n","                preds_json_ds = gr.Textbox(label=\"Top-k predictions (JSON)\", lines=8)\n","                preds_plot_ds = gr.Image(label=\"Scores\")\n","                infer_ds_btn.click(predict_from_dataset, [model_path, labels_path, action_sel, seq_idx, seq_len_ds], [preds_json_ds, preds_plot_ds])\n","\n","demo.launch(share=True, server_name=\"0.0.0.0\", show_error=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":611},"cellView":"form","id":"7O7iAM2i2cjS","executionInfo":{"status":"ok","timestamp":1761751811788,"user_tz":-570,"elapsed":1673,"user":{"displayName":"Avery Doan (AveryD)","userId":"04901609193817367419"}},"outputId":"6a91b10a-35e6-4547-80a2-86d73d428dfa"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://fc7445638997fff39c.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://fc7445638997fff39c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["## Version Main"],"metadata":{"id":"-OuQPoz6mjCl"}},{"cell_type":"code","source":["# @title\n","# === Gradio Action Detection App (real-time + landmark preview) ===\n","# !pip -q install gradio mediapipe opencv-python tensorflow scikit-learn matplotlib tqdm pillow\n","\n","import os, io, glob, json, time, shutil, collections, tempfile\n","from pathlib import Path\n","import numpy as np\n","import cv2\n","import gradio as gr\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix\n","import tensorflow as tf\n","from tensorflow.keras import layers, models\n","\n","# ---- Config / Paths ----\n","USE_DRIVE = False\n","BASE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Action Recognition\"  # your path\n","DATA_PATH = os.path.join(BASE_DIR, \"MP_Data\")\n","MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n","os.makedirs(DATA_PATH, exist_ok=True)\n","os.makedirs(MODELS_DIR, exist_ok=True)\n","\n","# ---- MediaPipe for keypoints & drawing ----\n","import mediapipe as mp\n","mp_holistic = mp.solutions.holistic\n","mp_drawing = mp.solutions.drawing_utils\n","mp_styles = mp.solutions.drawing_styles\n","\n","# ----------------- Landmark / Keypoint helpers -----------------\n","def extract_keypoints(results):\n","    def arr(vals, pad):\n","        a = np.array(vals).flatten()\n","        if a.size == 0:\n","            return np.zeros(pad)\n","        if a.size < pad:\n","            a = np.pad(a, (0, pad - a.size))\n","        return a\n","    pose = arr([[r.x, r.y, r.z, r.visibility] for r in (results.pose_landmarks.landmark if results.pose_landmarks else [])], 132)\n","    face = arr([[r.x, r.y, r.z] for r in (results.face_landmarks.landmark if results.face_landmarks else [])], 1404)\n","    lh   = arr([[r.x, r.y, r.z] for r in (results.left_hand_landmarks.landmark if results.left_hand_landmarks else [])], 63)\n","    rh   = arr([[r.x, r.y, r.z] for r in (results.right_hand_landmarks.landmark if results.right_hand_landmarks else [])], 63)\n","    return np.concatenate([pose, face, lh, rh])  # (1662,)\n","\n","def draw_landmarks_bgr(frame_bgr, results):\n","    \"\"\"Draw holistic landmarks on a BGR frame (in-place).\"\"\"\n","    mp_drawing.draw_landmarks(\n","        frame_bgr,\n","        results.face_landmarks,\n","        mp.solutions.holistic.FACEMESH_TESSELATION,\n","        landmark_drawing_spec=None,\n","        connection_drawing_spec=mp_styles.get_default_face_mesh_tesselation_style(),\n","    )\n","    mp_drawing.draw_landmarks(\n","        frame_bgr,\n","        results.pose_landmarks,\n","        mp.solutions.holistic.POSE_CONNECTIONS,\n","        landmark_drawing_spec=mp_styles.get_default_pose_landmarks_style(),\n","    )\n","    mp_drawing.draw_landmarks(\n","        frame_bgr,\n","        results.left_hand_landmarks,\n","        mp.solutions.holistic.HAND_CONNECTIONS,\n","        landmark_drawing_spec=mp_styles.get_default_hand_landmarks_style(),\n","        connection_drawing_spec=mp_styles.get_default_hand_connections_style(),\n","    )\n","    mp_drawing.draw_landmarks(\n","        frame_bgr,\n","        results.right_hand_landmarks,\n","        mp.solutions.holistic.HAND_CONNECTIONS,\n","        landmark_drawing_spec=mp_styles.get_default_hand_landmarks_style(),\n","        connection_drawing_spec=mp_styles.get_default_hand_connections_style(),\n","    )\n","\n","# ----------------- Video -> sequence -----------------\n","def video_to_sequence_keypoints(video_path, sequence_length=30, stride=1):\n","    cap = cv2.VideoCapture(video_path)\n","    frames = []\n","    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n","        i = 0\n","        while True:\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            if i % max(1, int(stride)) != 0:\n","                i += 1\n","                continue\n","            img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            img.flags.writeable = False\n","            results = holistic.process(img)\n","            img.flags.writeable = True\n","            frames.append(extract_keypoints(results))\n","            if len(frames) >= sequence_length:\n","                break\n","            i += 1\n","    cap.release()\n","    if len(frames) == 0:\n","        frames = [np.zeros(1662) for _ in range(sequence_length)]\n","    elif len(frames) < sequence_length:\n","        last = frames[-1]\n","        frames += [last] * (sequence_length - len(frames))\n","    else:\n","        frames = frames[:sequence_length]\n","    return np.stack(frames, axis=0)  # (T,1662)\n","\n","# ----------------- Annotate a clip with landmarks (NEW) -----------------\n","def annotate_video_with_landmarks(in_path, out_w=640, out_h=360, fps=24):\n","    \"\"\"\n","    Reads video file, draws holistic landmarks, writes an annotated .mp4 to a temp file.\n","    Returns output_path and a single preview frame (RGB ndarray).\n","    \"\"\"\n","    cap = cv2.VideoCapture(in_path)\n","    if not cap.isOpened():\n","        return None, None\n","    # Infer fps/size if available\n","    in_fps = cap.get(cv2.CAP_PROP_FPS)\n","    if in_fps and in_fps > 0:\n","        fps = int(in_fps)\n","    out_path = os.path.join(tempfile.gettempdir(), f\"annot_{int(time.time())}.mp4\")\n","    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n","    writer = cv2.VideoWriter(out_path, fourcc, fps, (out_w, out_h))\n","    preview = None\n","\n","    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n","        while True:\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            frame = cv2.resize(frame, (out_w, out_h))\n","            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            rgb.flags.writeable = False\n","            results = holistic.process(rgb)\n","            rgb.flags.writeable = True\n","            # draw on BGR\n","            draw_landmarks_bgr(frame, results)\n","            if preview is None:\n","                preview = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            writer.write(frame)\n","    cap.release()\n","    writer.release()\n","    return out_path, preview\n","\n","# ---- Image helpers ----\n","def fig_to_array(fig):\n","    buf = io.BytesIO()\n","    fig.savefig(buf, format=\"png\", bbox_inches=\"tight\")\n","    buf.seek(0)\n","    arr = np.array(Image.open(buf).convert(\"RGB\"))\n","    buf.close()\n","    return arr\n","\n","def plot_confmat(cm, labels):\n","    fig = plt.figure(figsize=(4 + 0.3 * len(labels), 4 + 0.3 * len(labels)))\n","    plt.imshow(cm, interpolation='nearest')\n","    plt.title(\"Confusion Matrix\")\n","    plt.colorbar()\n","    ticks = np.arange(len(labels))\n","    plt.xticks(ticks, labels, rotation=45, ha=\"right\")\n","    plt.yticks(ticks, labels)\n","    thresh = cm.max() / 2\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            plt.text(j, i, int(cm[i, j]), ha=\"center\", va=\"center\",\n","                     color=\"white\" if cm[i, j] > thresh else \"black\")\n","    plt.xlabel(\"Predicted\")\n","    plt.ylabel(\"True\")\n","    plt.tight_layout()\n","    arr = fig_to_array(fig)\n","    plt.close(fig)\n","    return arr\n","\n","# ---- Dataset & Model helpers (unchanged) ----\n","def list_actions():\n","    return sorted([d for d in os.listdir(DATA_PATH) if os.path.isdir(os.path.join(DATA_PATH, d))])\n","\n","def count_sequences(action):\n","    return len([p for p in glob.glob(os.path.join(DATA_PATH, action, \"*\")) if os.path.isdir(p)])\n","\n","def save_sequence(action, sequence_array):\n","    act_dir = os.path.join(DATA_PATH, action)\n","    os.makedirs(act_dir, exist_ok=True)\n","    next_id = count_sequences(action) + 1\n","    seq_dir = os.path.join(act_dir, str(next_id))\n","    os.makedirs(seq_dir, exist_ok=True)\n","    for i, f in enumerate(sequence_array):\n","        np.save(os.path.join(seq_dir, f\"{i}.npy\"), f)\n","    return next_id\n","\n","def load_dataset(actions=None, sequence_length=30):\n","    if not actions:\n","        actions = list_actions()\n","    a2i = {a: i for i, a in enumerate(actions)}\n","    X, y = [], []\n","    for a in actions:\n","        seq_dirs = sorted(\n","            [d for d in glob.glob(os.path.join(DATA_PATH, a, \"*\")) if os.path.isdir(d)],\n","            key=lambda p: int(os.path.basename(p)) if os.path.basename(p).isdigit() else 0\n","        )\n","        for sd in seq_dirs:\n","            frames = []\n","            for i in range(int(sequence_length)):\n","                fpath = os.path.join(sd, f\"{i}.npy\")\n","                frames.append(np.load(fpath) if os.path.exists(fpath) else np.zeros(1662))\n","            X.append(np.stack(frames))\n","            y.append(a2i[a])\n","    if len(X) == 0:\n","        return None, None, actions\n","    return np.array(X, dtype=np.float32), np.array(y, dtype=np.int64), actions\n","\n","def build_lstm_model(num_classes, sequence_length=30, feature_dim=1662,\n","                     lstm_units=128, dense_units=64, dropout=0.3, force_cpu=False):\n","    lstm_kwargs = dict(\n","        return_sequences=True,\n","        activation=\"tanh\",\n","        recurrent_activation=\"sigmoid\",\n","        implementation=2,\n","        recurrent_dropout=0.0,\n","        dropout=0.0,\n","    )\n","    inputs = layers.Input(shape=(sequence_length, feature_dim))\n","    x = layers.LSTM(lstm_units, **lstm_kwargs)(inputs)\n","    x = layers.Dropout(dropout)(x)\n","    x = layers.LSTM(lstm_units, activation=\"tanh\", recurrent_activation=\"sigmoid\",\n","                    implementation=2, recurrent_dropout=0.0, dropout=0.0)(x)\n","    x = layers.Dropout(dropout)(x)\n","    x = layers.Dense(dense_units, activation=\"relu\")(x)\n","    x = layers.Dropout(dropout)(x)\n","    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n","    model = models.Model(inputs, outputs)\n","    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","    return model\n","\n","# ==================== Tab 1 handlers (Collect) ====================\n","def create_action_folder(action_name_text, target_count):\n","    if not action_name_text or str(action_name_text).strip() == \"\":\n","        return \"Enter an action name.\", gr.update(value=\"\"), \"0 / 0\"\n","    p = os.path.join(DATA_PATH, action_name_text)\n","    os.makedirs(p, exist_ok=True)\n","    current = count_sequences(action_name_text)\n","    total = int(target_count)\n","    return f\"Using folder: {p}\", gr.update(value=action_name_text), f\"{current} / {total}\"\n","\n","# >>> CHANGED: now also returns an annotated preview video + preview frame\n","def record_and_save(action_name_text, video, sequence_length, stride, target_count):\n","    if not action_name_text or str(action_name_text).strip() == \"\":\n","        return \"Pick/create an action first.\", None, None, None\n","    if video is None:\n","        return \"Record or upload a short clip.\", None, None, None\n","\n","    seq = video_to_sequence_keypoints(video, sequence_length=int(sequence_length), stride=max(1, int(stride)))\n","    _ = save_sequence(action_name_text, seq)\n","    progress = f\"{count_sequences(action_name_text)} / {int(target_count)}\"\n","\n","    # Annotated preview for data collection quality check\n","    out_path, preview = annotate_video_with_landmarks(video)\n","    status = f\"Saved 1 sequence in '{action_name_text}'. Annotated preview generated.\"\n","    return status, progress, out_path, (preview if preview is not None else None)\n","\n","def reset_action(action_name_text, really=False):\n","    if not action_name_text or str(action_name_text).strip() == \"\":\n","        return \"Enter an action name.\", gr.update(value=\"\")\n","    if not really:\n","        return \"Tick the checkbox to confirm reset.\", gr.update(value=action_name_text)\n","    p = os.path.join(DATA_PATH, action_name_text)\n","    if os.path.exists(p):\n","        shutil.rmtree(p)\n","    os.makedirs(p, exist_ok=True)\n","    return f\"Cleared data for '{action_name_text}'.\", gr.update(value=action_name_text)\n","\n","# ==================== Tab 2 handler (Train/Evaluate) ====================\n","def train_model(sequence_length, test_size, epochs, batch_size):\n","    X, y, actions = load_dataset(sequence_length=int(sequence_length))\n","    if X is None:\n","        return \"Dataset empty. Collect sequences first.\", None, None, None\n","\n","    X = np.asarray(X, dtype=np.float32)\n","    y = np.asarray(y, dtype=np.int32)\n","    X[~np.isfinite(X)] = 0.0\n","\n","    mean = X.mean(axis=(0, 1), keepdims=True)\n","    std = X.std(axis=(0, 1), keepdims=True) + 1e-6\n","    X = (X - mean) / std\n","\n","    counts = np.bincount(y, minlength=len(actions))\n","    can_strat = (len(X) > len(actions)) and np.all(counts >= 2)\n","\n","    if can_strat:\n","        tsize = min(float(test_size), 0.2)\n","        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=tsize, random_state=42, stratify=y)\n","        val_data = (Xte, yte)\n","        msg = f\"Stratified split. Class counts: {dict(zip(actions, counts))}\"\n","    else:\n","        Xtr, ytr = X, y\n","        val_data = None\n","        msg = f\"No validation split (class counts: {dict(zip(actions, counts))}).\"\n","\n","    bs = max(1, min(int(batch_size), len(Xtr)))\n","    model = build_lstm_model(num_classes=len(actions), sequence_length=int(sequence_length))\n","    history = model.fit(Xtr, ytr, validation_data=val_data, epochs=int(epochs), batch_size=bs, verbose=0)\n","\n","    ts = int(time.time())\n","    model_path = os.path.join(MODELS_DIR, f\"lstm_{ts}.keras\")\n","    labels_path = os.path.join(MODELS_DIR, f\"lstm_{ts}.labels.json\")\n","    model.save(model_path)\n","    with open(labels_path, \"w\") as f:\n","        json.dump(actions, f)\n","\n","    fig = plt.figure(figsize=(8, 3))\n","    plt.plot(history.history.get(\"loss\", []), label=\"train\")\n","    if \"val_loss\" in history.history:\n","        plt.plot(history.history[\"val_loss\"], label=\"val\")\n","    plt.title(\"Loss\"); plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.legend()\n","    loss_arr = fig_to_array(fig); plt.close(fig)\n","\n","    if val_data is not None and len(val_data[0]) > 0:\n","        Xte, yte = val_data\n","        ypred = np.argmax(model.predict(Xte, verbose=0), axis=1)\n","        cm_arr = plot_confmat(confusion_matrix(yte, ypred), actions)\n","        report = classification_report(yte, ypred, target_names=actions)\n","    else:\n","        cm_arr = None\n","        report = \"No validation metrics (insufficient sequences per class). Collect ≥2 per action (10+ preferred).\"\n","\n","    summary = f\"{msg}\\nTrained on {len(Xtr)} sequences (batch_size={bs}).\\nSaved model:\\n{model_path}\\n{labels_path}\"\n","    return summary, loss_arr, cm_arr, report\n","\n","# ==================== Tab 3a handlers (Batch Test; same as before) ====================\n","def predict_from_webcam(model_path, labels_path, video, sequence_length, stride):\n","    if not model_path or not labels_path:\n","        return \"Provide model (.keras) and labels (.json).\", None\n","    if video is None:\n","        return \"Record a short clip.\", None\n","    model = tf.keras.models.load_model(model_path, compile=False)\n","    with open(labels_path, \"r\") as f:\n","        actions = json.load(f)\n","    seq = video_to_sequence_keypoints(video, sequence_length=int(sequence_length), stride=max(1, int(stride)))\n","    logits = model.predict(np.expand_dims(seq, 0), verbose=0)[0]\n","    idx = np.argsort(logits)[::-1]\n","    top = [(actions[i], float(logits[i])) for i in idx[:min(5, len(actions))]]\n","\n","    fig = plt.figure(figsize=(6, 3))\n","    labs = [t[0] for t in top]\n","    scores = [t[1] for t in top]\n","    plt.bar(labs, scores)\n","    plt.ylim(0, 1.0)\n","    plt.title(\"Top Predictions\")\n","    pred_arr = fig_to_array(fig)\n","    plt.close(fig)\n","    return json.dumps(top, indent=2), pred_arr\n","\n","def predict_from_dataset(model_path, labels_path, action_name_drop, seq_index, sequence_length):\n","    if not model_path or not labels_path:\n","        return \"Provide model (.keras) and labels (.json).\", None\n","    if not action_name_drop:\n","        return \"Select an action.\", None\n","\n","    seq_dir = os.path.join(DATA_PATH, action_name_drop, str(int(seq_index)))\n","    if not os.path.isdir(seq_dir):\n","        return f\"No sequence #{int(seq_index)} for '{action_name_drop}'.\", None\n","\n","    model = tf.keras.models.load_model(model_path, compile=False)\n","    with open(labels_path, \"r\") as f:\n","        actions = json.load(f)\n","\n","    frames = []\n","    for i in range(int(sequence_length)):\n","        fpath = os.path.join(seq_dir, f\"{i}.npy\")\n","        frames.append(np.load(fpath) if os.path.exists(fpath) else np.zeros(1662))\n","    seq = np.stack(frames)\n","\n","    logits = model.predict(np.expand_dims(seq, 0), verbose=0)[0]\n","    idx = np.argsort(logits)[::-1]\n","    top = [(actions[i], float(logits[i])) for i in idx[:min(5, len(actions))]]\n","\n","    fig = plt.figure(figsize=(6, 3))\n","    labs = [t[0] for t in top]\n","    scores = [t[1] for t in top]\n","    plt.bar(labs, scores)\n","    plt.ylim(0, 1.0)\n","    plt.title(\"Top Predictions (Dataset)\")\n","    pred_arr = fig_to_array(fig)\n","    plt.close(fig)\n","    return json.dumps(top, indent=2), pred_arr\n","\n","# ==================== Tab 3b handlers (NEW: Live Real-time) ====================\n","def load_model_and_labels(model_path, labels_path):\n","    model = tf.keras.models.load_model(model_path, compile=False)\n","    with open(labels_path, \"r\") as f:\n","        actions = json.load(f)\n","    return model, actions\n","\n","# Helper to draw prediction bars on frame\n","def draw_scores_bgr(frame, labels, probs, x=10, y=10, bar_w=180, bar_h=18, gap=6):\n","    if probs is None or labels is None:\n","        return frame\n","    for i, (lab, p) in enumerate(zip(labels, probs)):\n","        y0 = y + i*(bar_h+gap)\n","        cv2.rectangle(frame, (x, y0), (x+bar_w, y0+bar_h), (50,50,50), 1)\n","        w = int(bar_w * float(max(0.0, min(1.0, p))))\n","        cv2.rectangle(frame, (x, y0), (x+w, y0+bar_h), (0,200,0), -1)\n","        txt = f\"{lab}: {p:.2f}\"\n","        cv2.putText(frame, txt, (x+bar_w+10, y0+bar_h-4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)\n","    return frame\n","\n","# >>> NEW: stateful stream function\n","def realtime_stream(video_input, state, seq_len, top_k):\n","    \"\"\"\n","    video_input: Path to the video file recorded by gr.Video\n","    state: dict with keys {'deque': deque, 'model': None or model, 'labels': None or list}\n","    Returns: (annotated_frame, json_topk, updated_state)\n","    \"\"\"\n","    if video_input is None:\n","        return None, \"{}\", state\n","\n","    # initialize state\n","    if state is None or not isinstance(state, dict) or 'deque' not in state:\n","        state = {'deque': collections.deque(maxlen=int(seq_len)), 'model': None, 'labels': None}\n","\n","    # lazy-load model if paths provided in state\n","    model_path = state.get(\"model_path\")\n","    labels_path = state.get(\"labels_path\")\n","    if state.get(\"model\") is None and model_path and labels_path and os.path.exists(model_path) and os.path.exists(labels_path):\n","        model, labels = load_model_and_labels(model_path, labels_path)\n","        state[\"model\"] = model\n","        state[\"labels\"] = labels\n","\n","    cap = cv2.VideoCapture(video_input)\n","    annotated_frame = None\n","    top_json = \"{}\"\n","\n","    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n","        while cap.isOpened():\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","\n","            bgr = frame # frame is already BGR from cv2.VideoCapture\n","\n","            # MediaPipe pass\n","            rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n","            rgb.flags.writeable = False\n","            results = holistic.process(rgb)\n","            rgb.flags.writeable = True\n","            draw_landmarks_bgr(bgr, results)\n","            key = extract_keypoints(results)\n","            state['deque'].append(key)\n","\n","            if state.get(\"model\") is not None and len(state['deque']) >= int(seq_len):\n","                seq = np.stack(list(state['deque']))[-int(seq_len):]\n","                logits = state['model'].predict(np.expand_dims(seq, 0), verbose=0)[0]\n","                idx = np.argsort(logits)[::-1][:int(top_k)]\n","                labs = [state['labels'][i] for i in idx]\n","                probs = [float(logits[i]) for i in idx]\n","                # Draw overlay bars\n","                draw_scores_bgr(bgr, labs, probs, x=10, y=10)\n","                # Put top-1 big label\n","                cv2.putText(bgr, labs[0], (10, bgr.shape[0]-20), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,255,255), 2, cv2.LINE_AA)\n","                top_json = json.dumps(list(zip(labs, probs)), indent=2)\n","\n","            annotated_frame = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n","            # In a real streaming scenario, you would yield frames here.\n","            # For simplicity with gr.Video, we'll just process the whole clip\n","            # and return the last frame or an annotated video path.\n","            # Since the goal is *real-time*, we should aim to process frame-by-frame.\n","            # However, gr.Video's stream method is designed to pass the *file path* after recording finishes.\n","            # To do frame-by-frame, gr.Image(live=True) is the intended way.\n","            # Given that live=True is not working, let's reconsider the approach.\n","            # Perhaps the 'streaming' parameter on gr.Video is needed after all, or a different event.\n","\n","            # Let's revert to gr.Image with live=True and investigate why it failed.\n","            # The most common reason for Image.__init__() errors with 'live' is an old Gradio version.\n","            # Let's add a print statement to check the Gradio version.\n","\n","            pass # Process only the last frame for now with gr.Video\n","\n","    cap.release()\n","    # For gr.Video input, returning the last processed frame or an annotated video file path makes sense.\n","    # Let's return the last frame and the predictions.\n","\n","    return annotated_frame, top_json, state\n","\n","\n","# >>> NEW: helpers to set model paths into the live state\n","def set_live_model_paths(state, model_path, labels_path):\n","    if state is None or not isinstance(state, dict) or 'deque' not in state:\n","        state = {'deque': collections.deque(maxlen=30), 'model': None, 'labels': None}\n","    state['model_path'] = model_path\n","    state['labels_path'] = labels_path\n","    # reset loaded model so it reloads with new paths\n","    state['model'] = None\n","    state['labels'] = None\n","    return state, f\"Live model set:\\n{model_path}\\n{labels_path}\"\n","\n","# ==================== Gradio UI ====================\n","with gr.Blocks(title=\"Action Detection — Collect / Train / Test (Real-time)\") as demo:\n","    gr.Markdown(f\"**Base directory:** `{BASE_DIR}`  \\nKeypoints → `{DATA_PATH}`  \\nModels → `{MODELS_DIR}`\")\n","\n","    # Tab 1: Collect (now with landmark preview)\n","    with gr.Tab(\"1) Collect\"):\n","        with gr.Row():\n","            action_name_tb = gr.Textbox(label=\"Action name\", placeholder=\"e.g., wave, hello, thanks\")\n","            target_count = gr.Number(label=\"Target sequences\", value=30, precision=0)\n","            create_btn = gr.Button(\"Create / Select Folder\")\n","        status1 = gr.Markdown(\"\")\n","        progress = gr.Textbox(label=\"Progress\", value=\"0 / 30\", interactive=False)\n","\n","        gr.Markdown(\"**Record a short clip (5–10s) for ONE sequence**. We will save keypoints and also render an annotated preview with landmarks.\")\n","        with gr.Row():\n","            sequence_length = gr.Slider(10, 60, value=30, step=1, label=\"Frames per sequence\")\n","            stride = gr.Slider(1, 5, value=1, step=1, label=\"Stride (use every Nth frame)\")\n","        video_in = gr.Video(label=\"Video (webcam or upload)\", sources=[\"webcam\", \"upload\"], height=300)\n","        save_btn = gr.Button(\"Record & Save (one sequence)\")\n","\n","        # >>> NEW: annotated preview outputs\n","        anno_path = gr.Textbox(label=\"Annotated preview file (.mp4)\", interactive=False)\n","        anno_frame = gr.Image(label=\"Preview (first annotated frame)\", visible=True)\n","\n","        really = gr.Checkbox(label=\"Really reset this action's data?\")\n","        reset_btn = gr.Button(\"Reset Action Folder\", variant=\"stop\")\n","\n","        create_btn.click(create_action_folder, [action_name_tb, target_count], [status1, action_name_tb, progress])\n","        save_btn.click(\n","            record_and_save,\n","            [action_name_tb, video_in, sequence_length, stride, target_count],\n","            [status1, progress, anno_path, anno_frame]\n","        )\n","        reset_btn.click(reset_action, [action_name_tb, really], [status1, action_name_tb])\n","\n","    # Tab 2: Train / Evaluate\n","    with gr.Tab(\"2) Train / Evaluate\"):\n","        with gr.Row():\n","            seq_len_train = gr.Slider(10, 60, value=30, step=1, label=\"Sequence length\")\n","            test_size = gr.Slider(0.1, 0.5, value=0.2, step=0.05, label=\"Test size\")\n","        with gr.Row():\n","            epochs = gr.Slider(1, 50, value=12, step=1, label=\"Epochs\")\n","            batch = gr.Slider(4, 64, value=16, step=4, label=\"Batch size\")\n","        train_btn = gr.Button(\"Train LSTM\")\n","        train_msg = gr.Textbox(label=\"Summary\", lines=4)\n","        loss_img = gr.Image(label=\"Training Loss\")\n","        cm_img = gr.Image(label=\"Confusion Matrix\")\n","        report_txt = gr.Textbox(label=\"Classification Report\", lines=14)\n","        train_btn.click(train_model, [seq_len_train, test_size, epochs, batch], [train_msg, loss_img, cm_img, report_txt])\n","\n","    # Tab 3a: Batch Test (existing)\n","    with gr.Tab(\"3a) Test (clip based)\"):\n","        gr.Markdown(\"Test with a short **webcam/uploaded clip** or an existing **dataset sequence**.\")\n","        with gr.Row():\n","            with gr.Column():\n","                gr.Markdown(\"**Webcam/Upload Clip Test**\")\n","                model_path = gr.Textbox(label=\"Model .keras path\")\n","                labels_path = gr.Textbox(label=\"Labels .json path\")\n","                seq_len_inf = gr.Slider(10, 60, value=30, step=1, label=\"Sequence length\")\n","                stride_inf = gr.Slider(1, 5, value=1, step=1, label=\"Stride\")\n","                vid_inf = gr.Video(label=\"Video (webcam or upload)\", sources=[\"webcam\", \"upload\"], height=300)\n","                infer_btn = gr.Button(\"Predict (Clip)\")\n","                preds_json = gr.Textbox(label=\"Top-k predictions (JSON)\", lines=8)\n","                preds_plot = gr.Image(label=\"Scores\")\n","                infer_btn.click(predict_from_webcam, [model_path, labels_path, vid_inf, seq_len_inf, stride_inf], [preds_json, preds_plot])\n","\n","            with gr.Column():\n","                gr.Markdown(\"**Dataset Test**\")\n","                action_sel = gr.Dropdown(label=\"Action\", choices=list_actions())\n","                seq_idx = gr.Number(label=\"Sequence #\", value=1, precision=0)\n","                seq_len_ds = gr.Slider(10, 60, value=30, step=1, label=\"Sequence length\")\n","                infer_ds_btn = gr.Button(\"Predict (Dataset)\")\n","                preds_json_ds = gr.Textbox(label=\"Top-k predictions (JSON)\", lines=8)\n","                preds_plot_ds = gr.Image(label=\"Scores\")\n","                infer_ds_btn.click(predict_from_dataset, [model_path, labels_path, action_sel, seq_idx, seq_len_ds], [preds_json_ds, preds_plot_ds])\n","\n","    # >>> NEW Tab 3b: True Real-time (streaming)\n","    with gr.Tab(\"3b) Live Test (real-time)\"):\n","        gr.Markdown(\"Live, frame-by-frame predictions with a sliding window and on-frame landmarks & scores.\")\n","        live_state = gr.State({'deque': collections.deque(maxlen=30), 'model': None, 'labels': None})\n","\n","        with gr.Row():\n","            live_model_path = gr.Textbox(label=\"Model .keras path (for live)\")\n","            live_labels_path = gr.Textbox(label=\"Labels .json path (for live)\")\n","            seq_len_live = gr.Slider(10, 60, value=30, step=1, label=\"Sequence length (window)\")\n","            topk_live = gr.Slider(1, 5, value=3, step=1, label=\"Top-K to display\")\n","\n","        set_live_btn = gr.Button(\"Use these paths for LIVE\")\n","        set_msg = gr.Textbox(label=\"Status\", interactive=False)\n","        set_live_btn.click(set_live_model_paths, [live_state, live_model_path, live_labels_path], [live_state, set_msg])\n","\n","        # Attempting gr.Video with sources=[\"webcam\"] instead of gr.Image(live=True)\n","        cam_live = gr.Video(label=\"Webcam (live)\", sources=[\"webcam\"], height=360)\n","        out_live = gr.Image(label=\"Annotated (live)\")\n","        json_live = gr.Textbox(label=\"Top-K (JSON)\", lines=6)\n","\n","        # Stream frames from gr.Video → model\n","        # cam_live.stream(  # This caused AttributeError\n","        #     fn=realtime_stream,\n","        #     inputs=[cam_live, live_state, seq_len_live, topk_live],\n","        #     outputs=[out_live, json_live, live_state],\n","        #     time_limit=None\n","        # )\n","\n","        # Since stream() is not available, we'll disable the live tab for now or need a different approach\n","        gr.Markdown(\"Real-time streaming is not supported with the current Gradio version/setup.\")\n","\n","\n","demo.launch(share=True, server_name=\"0.0.0.0\", show_error=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":611},"cellView":"form","id":"GLL92if_2hpC","executionInfo":{"status":"ok","timestamp":1761751608258,"user_tz":-570,"elapsed":1060,"user":{"displayName":"Avery Doan (AveryD)","userId":"04901609193817367419"}},"outputId":"c2c76e74-a571-4e8f-c350-975552a52a43"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://515514c0bc916c7f04.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://515514c0bc916c7f04.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["# VERSION 3"],"metadata":{"id":"Ihc9xHK8mY8y"}},{"cell_type":"code","source":["# @title\n","# === Gradio Action Detection App (real-time + landmark preview) ===\n","# !pip -q install gradio mediapipe opencv-python tensorflow scikit-learn matplotlib tqdm pillow\n","\n","import os, io, glob, json, time, shutil, collections, tempfile\n","from pathlib import Path\n","import numpy as np\n","import cv2\n","import gradio as gr\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix\n","import tensorflow as tf\n","from tensorflow.keras import layers, models\n","\n","# ---- Config / Paths ----\n","USE_DRIVE = False\n","BASE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Action Recognition\"  # your path\n","DATA_PATH = os.path.join(BASE_DIR, \"MP_Data\")\n","MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n","os.makedirs(DATA_PATH, exist_ok=True)\n","os.makedirs(MODELS_DIR, exist_ok=True)\n","\n","# ---- MediaPipe for keypoints & drawing ----\n","import mediapipe as mp\n","mp_holistic = mp.solutions.holistic\n","mp_drawing = mp.solutions.drawing_utils\n","mp_styles = mp.solutions.drawing_styles\n","\n","# ----------------- Landmark / Keypoint helpers -----------------\n","def extract_keypoints(results):\n","    def arr(vals, pad):\n","        a = np.array(vals).flatten()\n","        if a.size == 0:\n","            return np.zeros(pad)\n","        if a.size < pad:\n","            a = np.pad(a, (0, pad - a.size))\n","        return a\n","    pose = arr([[r.x, r.y, r.z, r.visibility] for r in (results.pose_landmarks.landmark if results.pose_landmarks else [])], 132)\n","    face = arr([[r.x, r.y, r.z] for r in (results.face_landmarks.landmark if results.face_landmarks else [])], 1404)\n","    lh   = arr([[r.x, r.y, r.z] for r in (results.left_hand_landmarks.landmark if results.left_hand_landmarks else [])], 63)\n","    rh   = arr([[r.x, r.y, r.z] for r in (results.right_hand_landmarks.landmark if results.right_hand_landmarks else [])], 63)\n","    return np.concatenate([pose, face, lh, rh])  # (1662,)\n","\n","def draw_landmarks_bgr(frame_bgr, results):\n","    \"\"\"Draw holistic landmarks on a BGR frame (in-place).\"\"\"\n","    mp_drawing.draw_landmarks(\n","        frame_bgr,\n","        results.face_landmarks,\n","        mp.solutions.holistic.FACEMESH_TESSELATION,\n","        landmark_drawing_spec=None,\n","        connection_drawing_spec=mp_styles.get_default_face_mesh_tesselation_style(),\n","    )\n","    mp_drawing.draw_landmarks(\n","        frame_bgr,\n","        results.pose_landmarks,\n","        mp.solutions.holistic.POSE_CONNECTIONS,\n","        landmark_drawing_spec=mp_styles.get_default_pose_landmarks_style(),\n","    )\n","    mp_drawing.draw_landmarks(\n","        frame_bgr,\n","        results.left_hand_landmarks,\n","        mp.solutions.holistic.HAND_CONNECTIONS,\n","        landmark_drawing_spec=mp_styles.get_default_hand_landmarks_style(),\n","        connection_drawing_spec=mp_styles.get_default_hand_connections_style(),\n","    )\n","    mp_drawing.draw_landmarks(\n","        frame_bgr,\n","        results.right_hand_landmarks,\n","        mp.solutions.holistic.HAND_CONNECTIONS,\n","        landmark_drawing_spec=mp_styles.get_default_hand_landmarks_style(),\n","        connection_drawing_spec=mp_styles.get_default_hand_connections_style(),\n","    )\n","\n","# ----------------- Video -> sequence -----------------\n","def video_to_sequence_keypoints(video_path, sequence_length=30, stride=1):\n","    cap = cv2.VideoCapture(video_path)\n","    frames = []\n","    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n","        i = 0\n","        while True:\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            if i % max(1, int(stride)) != 0:\n","                i += 1\n","                continue\n","            img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            img.flags.writeable = False\n","            results = holistic.process(img)\n","            img.flags.writeable = True\n","            frames.append(extract_keypoints(results))\n","            if len(frames) >= sequence_length:\n","                break\n","            i += 1\n","    cap.release()\n","    if len(frames) == 0:\n","        frames = [np.zeros(1662) for _ in range(sequence_length)]\n","    elif len(frames) < sequence_length:\n","        last = frames[-1]\n","        frames += [last] * (sequence_length - len(frames))\n","    else:\n","        frames = frames[:sequence_length]\n","    return np.stack(frames, axis=0)  # (T,1662)\n","\n","# ----------------- Annotate a clip with landmarks (NEW) -----------------\n","def annotate_video_with_landmarks(in_path, out_w=640, out_h=360, fps=24):\n","    \"\"\"\n","    Reads video file, draws holistic landmarks, writes an annotated .mp4 to a temp file.\n","    Returns output_path and a single preview frame (RGB ndarray).\n","    \"\"\"\n","    cap = cv2.VideoCapture(in_path)\n","    if not cap.isOpened():\n","        return None, None\n","    # Infer fps/size if available\n","    in_fps = cap.get(cv2.CAP_PROP_FPS)\n","    if in_fps and in_fps > 0:\n","        fps = int(in_fps)\n","    out_path = os.path.join(tempfile.gettempdir(), f\"annot_{int(time.time())}.mp4\")\n","    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n","    writer = cv2.VideoWriter(out_path, fourcc, fps, (out_w, out_h))\n","    preview = None\n","\n","    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n","        while True:\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            frame = cv2.resize(frame, (out_w, out_h))\n","            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            rgb.flags.writeable = False\n","            results = holistic.process(rgb)\n","            rgb.flags.writeable = True\n","            # draw on BGR\n","            draw_landmarks_bgr(frame, results)\n","            if preview is None:\n","                preview = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            writer.write(frame)\n","    cap.release()\n","    writer.release()\n","    return out_path, preview\n","\n","# ---- Image helpers ----\n","def fig_to_array(fig):\n","    buf = io.BytesIO()\n","    fig.savefig(buf, format=\"png\", bbox_inches=\"tight\")\n","    buf.seek(0)\n","    arr = np.array(Image.open(buf).convert(\"RGB\"))\n","    buf.close()\n","    return arr\n","\n","def plot_confmat(cm, labels):\n","    fig = plt.figure(figsize=(4 + 0.3 * len(labels), 4 + 0.3 * len(labels)))\n","    plt.imshow(cm, interpolation='nearest')\n","    plt.title(\"Confusion Matrix\")\n","    plt.colorbar()\n","    ticks = np.arange(len(labels))\n","    plt.xticks(ticks, labels, rotation=45, ha=\"right\")\n","    plt.yticks(ticks, labels)\n","    thresh = cm.max() / 2\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            plt.text(j, i, int(cm[i, j]), ha=\"center\", va=\"center\",\n","                     color=\"white\" if cm[i, j] > thresh else \"black\")\n","    plt.xlabel(\"Predicted\")\n","    plt.ylabel(\"True\")\n","    plt.tight_layout()\n","    arr = fig_to_array(fig)\n","    plt.close(fig)\n","    return arr\n","\n","# ---- Dataset & Model helpers (unchanged) ----\n","def list_actions():\n","    return sorted([d for d in os.listdir(DATA_PATH) if os.path.isdir(os.path.join(DATA_PATH, d))])\n","\n","def count_sequences(action):\n","    return len([p for p in glob.glob(os.path.join(DATA_PATH, action, \"*\")) if os.path.isdir(p)])\n","\n","def save_sequence(action, sequence_array):\n","    act_dir = os.path.join(DATA_PATH, action)\n","    os.makedirs(act_dir, exist_ok=True)\n","    next_id = count_sequences(action) + 1\n","    seq_dir = os.path.join(act_dir, str(next_id))\n","    os.makedirs(seq_dir, exist_ok=True)\n","    for i, f in enumerate(sequence_array):\n","        np.save(os.path.join(seq_dir, f\"{i}.npy\"), f)\n","    return next_id\n","\n","def load_dataset(actions=None, sequence_length=30):\n","    if not actions:\n","        actions = list_actions()\n","    a2i = {a: i for i, a in enumerate(actions)}\n","    X, y = [], []\n","    for a in actions:\n","        seq_dirs = sorted(\n","            [d for d in glob.glob(os.path.join(DATA_PATH, a, \"*\")) if os.path.isdir(d)],\n","            key=lambda p: int(os.path.basename(p)) if os.path.basename(p).isdigit() else 0\n","        )\n","        for sd in seq_dirs:\n","            frames = []\n","            for i in range(int(sequence_length)):\n","                fpath = os.path.join(sd, f\"{i}.npy\")\n","                frames.append(np.load(fpath) if os.path.exists(fpath) else np.zeros(1662))\n","            X.append(np.stack(frames))\n","            y.append(a2i[a])\n","    if len(X) == 0:\n","        return None, None, actions\n","    return np.array(X, dtype=np.float32), np.array(y, dtype=np.int64), actions\n","\n","def build_lstm_model(num_classes, sequence_length=30, feature_dim=1662,\n","                     lstm_units=128, dense_units=64, dropout=0.3, force_cpu=False):\n","    lstm_kwargs = dict(\n","        return_sequences=True,\n","        activation=\"tanh\",\n","        recurrent_activation=\"sigmoid\",\n","        implementation=2,\n","        recurrent_dropout=0.0,\n","        dropout=0.0,\n","    )\n","    inputs = layers.Input(shape=(sequence_length, feature_dim))\n","    x = layers.LSTM(lstm_units, **lstm_kwargs)(inputs)\n","    x = layers.Dropout(dropout)(x)\n","    x = layers.LSTM(lstm_units, activation=\"tanh\", recurrent_activation=\"sigmoid\",\n","                    implementation=2, recurrent_dropout=0.0, dropout=0.0)(x)\n","    x = layers.Dropout(dropout)(x)\n","    x = layers.Dense(dense_units, activation=\"relu\")(x)\n","    x = layers.Dropout(dropout)(x)\n","    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n","    model = models.Model(inputs, outputs)\n","    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","    return model\n","\n","# ==================== Tab 1 handlers (Collect) ====================\n","def create_action_folder(action_name_text, target_count):\n","    if not action_name_text or str(action_name_text).strip() == \"\":\n","        return \"Enter an action name.\", gr.update(value=\"\"), \"0 / 0\"\n","    p = os.path.join(DATA_PATH, action_name_text)\n","    os.makedirs(p, exist_ok=True)\n","    current = count_sequences(action_name_text)\n","    total = int(target_count)\n","    return f\"Using folder: {p}\", gr.update(value=action_name_text), f\"{current} / {total}\"\n","\n","# >>> CHANGED: now also returns an annotated preview video + preview frame\n","def record_and_save(action_name_text, video, sequence_length, stride, target_count):\n","    if not action_name_text or str(action_name_text).strip() == \"\":\n","        return \"Pick/create an action first.\", None, None, None\n","    if video is None:\n","        return \"Record or upload a short clip.\", None, None, None\n","\n","    seq = video_to_sequence_keypoints(video, sequence_length=int(sequence_length), stride=max(1, int(stride)))\n","    _ = save_sequence(action_name_text, seq)\n","    progress = f\"{count_sequences(action_name_text)} / {int(target_count)}\"\n","\n","    # Annotated preview for data collection quality check\n","    out_path, preview = annotate_video_with_landmarks(video)\n","    status = f\"Saved 1 sequence in '{action_name_text}'. Annotated preview generated.\"\n","    return status, progress, out_path, (preview if preview is not None else None)\n","\n","def reset_action(action_name_text, really=False):\n","    if not action_name_text or str(action_name_text).strip() == \"\":\n","        return \"Enter an action name.\", gr.update(value=\"\")\n","    if not really:\n","        return \"Tick the checkbox to confirm reset.\", gr.update(value=action_name_text)\n","    p = os.path.join(DATA_PATH, action_name_text)\n","    if os.path.exists(p):\n","        shutil.rmtree(p)\n","    os.makedirs(p, exist_ok=True)\n","    return f\"Cleared data for '{action_name_text}'.\", gr.update(value=action_name_text)\n","\n","# ==================== Tab 2 handler (Train/Evaluate) ====================\n","def train_model(sequence_length, test_size, epochs, batch_size):\n","    X, y, actions = load_dataset(sequence_length=int(sequence_length))\n","    if X is None:\n","        return \"Dataset empty. Collect sequences first.\", None, None, None\n","\n","    # --- sanitize & normalize (same as before) ---\n","    X = np.asarray(X, dtype=np.float32)\n","    y = np.asarray(y, dtype=np.int32)\n","    X[~np.isfinite(X)] = 0.0\n","\n","    mean = X.mean(axis=(0, 1), keepdims=True)      # shape (1,1,1662)\n","    std  = X.std(axis=(0, 1), keepdims=True) + 1e-6\n","    X = (X - mean) / std\n","\n","    counts = np.bincount(y, minlength=len(actions))\n","    can_strat = (len(X) > len(actions)) and np.all(counts >= 2)\n","\n","    if can_strat:\n","        tsize = min(float(test_size), 0.2)\n","        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=tsize, random_state=42, stratify=y)\n","        val_data = (Xte, yte)\n","        msg = f\"Stratified split. Class counts: {dict(zip(actions, counts))}\"\n","    else:\n","        Xtr, ytr = X, y\n","        val_data = None\n","        msg = f\"No validation split (class counts: {dict(zip(actions, counts))}).\"\n","\n","    bs = max(1, min(int(batch_size), len(Xtr)))\n","\n","    model = build_lstm_model(num_classes=len(actions), sequence_length=int(sequence_length))\n","    history = model.fit(Xtr, ytr, validation_data=val_data, epochs=int(epochs), batch_size=bs, verbose=0)\n","\n","    # --- Save artifacts IN THIS ORDER ---\n","    ts = int(time.time())\n","    model_path  = os.path.join(MODELS_DIR, f\"lstm_{ts}.keras\")\n","    labels_path = os.path.join(MODELS_DIR, f\"lstm_{ts}.labels.json\")\n","    model.save(model_path)\n","    with open(labels_path, \"w\") as f:\n","        json.dump(actions, f)\n","\n","    # normalization bundle (must happen AFTER ts defined)\n","    norm_path = os.path.join(MODELS_DIR, f\"lstm_{ts}.norm.npz\")\n","    np.savez_compressed(\n","        norm_path,\n","        mean=mean.astype(np.float32),\n","        std=std.astype(np.float32),\n","        sequence_length=int(sequence_length)\n","    )\n","\n","    # --- Plots & metrics ---\n","    fig = plt.figure(figsize=(8, 3))\n","    plt.plot(history.history.get(\"loss\", []), label=\"train\")\n","    if \"val_loss\" in history.history:\n","        plt.plot(history.history[\"val_loss\"], label=\"val\")\n","    plt.title(\"Loss\"); plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.legend()\n","    loss_arr = fig_to_array(fig); plt.close(fig)\n","\n","    if val_data is not None and len(val_data[0]) > 0:\n","        Xte, yte = val_data\n","        ypred = np.argmax(model.predict(Xte, verbose=0), axis=1)\n","        cm_arr = plot_confmat(confusion_matrix(yte, ypred), actions)\n","        report = classification_report(yte, ypred, target_names=actions)\n","    else:\n","        cm_arr = None\n","        report = \"No validation metrics (insufficient sequences per class). Collect ≥2 per action (10+ preferred).\"\n","\n","    summary = (f\"{msg}\\nTrained on {len(Xtr)} sequences (batch_size={bs}).\\n\"\n","               f\"Saved model:\\n{model_path}\\n{labels_path}\\n{norm_path}\")\n","    return summary, loss_arr, cm_arr, report\n","\n","\n","\n","\n","# ==================== Tab 3a handlers (Batch Test; same as before) ====================\n","def _load_model_bundle(model_path, labels_path):\n","    model = tf.keras.models.load_model(model_path, compile=False)\n","    with open(labels_path, \"r\") as f:\n","        actions = json.load(f)\n","    # try to find matching normalization file\n","    base = os.path.splitext(os.path.basename(model_path))[0]   # e.g., \"lstm_173023...\"\n","    norm_guess = os.path.join(os.path.dirname(model_path), f\"{base}.norm.npz\")\n","    norm = None\n","    if os.path.exists(norm_guess):\n","        norm = np.load(norm_guess)\n","    return model, actions, norm\n","\n","def _apply_norm(seq, norm):\n","    if norm is None:\n","        return seq\n","    mean = norm[\"mean\"]  # (1,1,1662)\n","    std  = norm[\"std\"]   # (1,1,1662)\n","    # reshape to (1,1662) so (T,1662) - (1,1662) broadcasts cleanly\n","    mean2 = mean.reshape(1, -1).astype(np.float32)\n","    std2  = std.reshape(1, -1).astype(np.float32)\n","    return (seq - mean2) / (std2 + 1e-6)\n","\n","\n","def _nonzero_landmark_ratio(seq):\n","    # seq shape: (T, 1662). Count frames with any nonzero landmark.\n","    nonzero = (np.abs(seq).sum(axis=1) > 1e-8).astype(np.float32)\n","    return float(nonzero.mean()), int(nonzero.sum()), int(seq.shape[0])\n","\n","def _predict_seq(model, actions, seq, conf_floor=0.40, topk=5):\n","    logits = model.predict(np.expand_dims(seq, 0), verbose=0)[0]\n","    idx = np.argsort(logits)[::-1]\n","    idx = idx[:min(topk, len(actions))]\n","    top = [(actions[i], float(logits[i])) for i in idx]\n","    max_lab, max_prob = top[0]\n","    status = \"ok\" if max_prob >= conf_floor else \"low_confidence\"\n","    return top, status\n","\n","def predict_from_webcam(model_path, labels_path, video, sequence_length, stride):\n","    if not model_path or not labels_path:\n","        return \"Provide model (.keras) and labels (.json).\", None\n","    if video is None:\n","        return \"Record a short clip.\", None\n","\n","    model, actions, norm = _load_model_bundle(model_path, labels_path)\n","\n","    # Basic sanity: model output vs labels\n","    out_dim = model.output_shape[-1]\n","    if out_dim != len(actions):\n","        return (f\"Label/model mismatch: model has {out_dim} outputs but {len(actions)} labels. \"\n","                \"Make sure the .labels.json matches this .keras.\"), None\n","\n","    # Build sequence\n","    seq = video_to_sequence_keypoints(video, sequence_length=int(sequence_length),\n","                                      stride=max(1, int(stride)))\n","\n","    # If we saved a different training sequence length, adapt gracefully\n","    if norm is not None and \"sequence_length\" in norm.files:\n","        train_T = int(norm[\"sequence_length\"])\n","        if seq.shape[0] != train_T:\n","            # center-crop or pad with last frame to match train length\n","            if seq.shape[0] > train_T:\n","                start = max(0, (seq.shape[0]-train_T)//2)\n","                seq = seq[start:start+train_T]\n","            else:\n","                pad_n = train_T - seq.shape[0]\n","                seq = np.concatenate([seq, np.repeat(seq[-1][None, :], pad_n, axis=0)], axis=0)\n","\n","    # Apply same normalization\n","    seq = _apply_norm(seq.astype(np.float32), norm)\n","\n","    # Diagnostics: landmark presence\n","    nz_ratio, nz_frames, total_frames = _nonzero_landmark_ratio(seq)\n","    diag = {\"nonzero_ratio\": round(nz_ratio, 3),\n","            \"nonzero_frames\": nz_frames,\n","            \"total_frames\": total_frames}\n","\n","    # Predict\n","    top, status = _predict_seq(model, actions, seq, conf_floor=0.40, topk=min(5, len(actions)))\n","    diag[\"status\"] = status\n","    diag[\"top1\"] = {\"label\": top[0][0], \"prob\": round(top[0][1], 3)}\n","\n","    # Plot bars\n","    fig = plt.figure(figsize=(6, 3))\n","    labs = [t[0] for t in top]\n","    scores = [t[1] for t in top]\n","    plt.bar(labs, scores)\n","    plt.ylim(0, 1.0)\n","    plt.title(\"Top Predictions\")\n","    pred_arr = fig_to_array(fig); plt.close(fig)\n","\n","    return json.dumps({\"preds\": top, \"diag\": diag}, indent=2), pred_arr\n","\n","\n","def predict_from_dataset(model_path, labels_path, action_name_drop, seq_index, sequence_length):\n","    if not model_path or not labels_path:\n","        return \"Provide model (.keras) and labels (.json).\", None\n","    if not action_name_drop:\n","        return \"Select an action.\", None\n","\n","    seq_dir = os.path.join(DATA_PATH, action_name_drop, str(int(seq_index)))\n","    if not os.path.isdir(seq_dir):\n","        return f\"No sequence #{int(seq_index)} for '{action_name_drop}'.\", None\n","\n","    model, actions, norm = _load_model_bundle(model_path, labels_path)\n","\n","    out_dim = model.output_shape[-1]\n","    if out_dim != len(actions):\n","        return (f\"Label/model mismatch: model has {out_dim} outputs but {len(actions)} labels. \"\n","                \"Make sure the .labels.json matches this .keras.\"), None\n","\n","    # Load saved frames\n","    frames = []\n","    for i in range(int(sequence_length)):\n","        fpath = os.path.join(seq_dir, f\"{i}.npy\")\n","        frames.append(np.load(fpath) if os.path.exists(fpath) else np.zeros(1662))\n","    seq = np.stack(frames).astype(np.float32)\n","\n","    # Align to train sequence length if needed\n","    if norm is not None and \"sequence_length\" in norm.files:\n","        train_T = int(norm[\"sequence_length\"])\n","        if seq.shape[0] != train_T:\n","            if seq.shape[0] > train_T:\n","                start = max(0, (seq.shape[0]-train_T)//2)\n","                seq = seq[start:start+train_T]\n","            else:\n","                pad_n = train_T - seq.shape[0]\n","                seq = np.concatenate([seq, np.repeat(seq[-1][None, :], pad_n, axis=0)], axis=0)\n","\n","    seq = _apply_norm(seq, norm)\n","\n","    nz_ratio, nz_frames, total_frames = _nonzero_landmark_ratio(seq)\n","    diag = {\"nonzero_ratio\": round(nz_ratio, 3),\n","            \"nonzero_frames\": nz_frames,\n","            \"total_frames\": total_frames}\n","\n","    top, status = _predict_seq(model, actions, seq, conf_floor=0.40, topk=min(5, len(actions)))\n","    diag[\"status\"] = status\n","    diag[\"top1\"] = {\"label\": top[0][0], \"prob\": round(top[0][1], 3)}\n","\n","    fig = plt.figure(figsize=(6, 3))\n","    labs = [t[0] for t in top]\n","    scores = [t[1] for t in top]\n","    plt.bar(labs, scores)\n","    plt.ylim(0, 1.0)\n","    plt.title(\"Top Predictions (Dataset)\")\n","    pred_arr = fig_to_array(fig); plt.close(fig)\n","    return json.dumps({\"preds\": top, \"diag\": diag}, indent=2), pred_arr\n","\n","\n","# ==================== Tab 3b handlers (NEW: Live Real-time) ====================\n","def load_model_and_labels(model_path, labels_path):\n","    model = tf.keras.models.load_model(model_path, compile=False)\n","    with open(labels_path, \"r\") as f:\n","        actions = json.load(f)\n","    return model, actions\n","\n","# Helper to draw prediction bars on frame\n","def draw_scores_bgr(frame, labels, probs, x=10, y=10, bar_w=180, bar_h=18, gap=6):\n","    if probs is None or labels is None:\n","        return frame\n","    for i, (lab, p) in enumerate(zip(labels, probs)):\n","        y0 = y + i*(bar_h+gap)\n","        cv2.rectangle(frame, (x, y0), (x+bar_w, y0+bar_h), (50,50,50), 1)\n","        w = int(bar_w * float(max(0.0, min(1.0, p))))\n","        cv2.rectangle(frame, (x, y0), (x+w, y0+bar_h), (0,200,0), -1)\n","        txt = f\"{lab}: {p:.2f}\"\n","        cv2.putText(frame, txt, (x+bar_w+10, y0+bar_h-4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)\n","    return frame\n","\n","# >>> NEW: stateful stream function\n","def realtime_stream(video_input, state, seq_len, top_k):\n","    \"\"\"\n","    video_input: Path to the video file recorded by gr.Video\n","    state: dict with keys {'deque': deque, 'model': None or model, 'labels': None or list}\n","    Returns: (annotated_frame, json_topk, updated_state)\n","    \"\"\"\n","    if video_input is None:\n","        return None, \"{}\", state\n","\n","    # initialize state\n","    if state is None or not isinstance(state, dict) or 'deque' not in state:\n","        state = {'deque': collections.deque(maxlen=int(seq_len)), 'model': None, 'labels': None}\n","\n","    # lazy-load model if paths provided in state\n","    model_path = state.get(\"model_path\")\n","    labels_path = state.get(\"labels_path\")\n","    if state.get(\"model\") is None and model_path and labels_path and os.path.exists(model_path) and os.path.exists(labels_path):\n","        model, labels = load_model_and_labels(model_path, labels_path)\n","        state[\"model\"] = model\n","        state[\"labels\"] = labels\n","\n","    cap = cv2.VideoCapture(video_input)\n","    annotated_frame = None\n","    top_json = \"{}\"\n","\n","    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n","        while cap.isOpened():\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","\n","            bgr = frame # frame is already BGR from cv2.VideoCapture\n","\n","            # MediaPipe pass\n","            rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n","            rgb.flags.writeable = False\n","            results = holistic.process(rgb)\n","            rgb.flags.writeable = True\n","            draw_landmarks_bgr(bgr, results)\n","            key = extract_keypoints(results)\n","            state['deque'].append(key)\n","\n","            if state.get(\"model\") is not None and len(state['deque']) >= int(seq_len):\n","                seq = np.stack(list(state['deque']))[-int(seq_len):]\n","                logits = state['model'].predict(np.expand_dims(seq, 0), verbose=0)[0]\n","                idx = np.argsort(logits)[::-1][:int(top_k)]\n","                labs = [state['labels'][i] for i in idx]\n","                probs = [float(logits[i]) for i in idx]\n","                # Draw overlay bars\n","                draw_scores_bgr(bgr, labs, probs, x=10, y=10)\n","                # Put top-1 big label\n","                cv2.putText(bgr, labs[0], (10, bgr.shape[0]-20), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,255,255), 2, cv2.LINE_AA)\n","                top_json = json.dumps(list(zip(labs, probs)), indent=2)\n","\n","            annotated_frame = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n","            # In a real streaming scenario, you would yield frames here.\n","            # For simplicity with gr.Video, we'll just process the whole clip\n","            # and return the last frame or an annotated video path.\n","            # Since the goal is *real-time*, we should aim to process frame-by-frame.\n","            # However, gr.Video's stream method is designed to pass the *file path* after recording finishes.\n","            # To do frame-by-frame, gr.Image(live=True) is the intended way.\n","            # Given that live=True is not working, let's reconsider the approach.\n","            # Perhaps the 'streaming' parameter on gr.Video is needed after all, or a different event.\n","\n","            # Let's revert to gr.Image with live=True and investigate why it failed.\n","            # The most common reason for Image.__init__() errors with 'live' is an old Gradio version.\n","            # Let's add a print statement to check the Gradio version.\n","\n","            pass # Process only the last frame for now with gr.Video\n","\n","    cap.release()\n","    # For gr.Video input, returning the last processed frame or an annotated video file path makes sense.\n","    # Let's return the last frame and the predictions.\n","\n","    return annotated_frame, top_json, state\n","\n","\n","# >>> NEW: helpers to set model paths into the live state\n","def set_live_model_paths(state, model_path, labels_path):\n","    if state is None or not isinstance(state, dict) or 'deque' not in state:\n","        state = {'deque': collections.deque(maxlen=30), 'model': None, 'labels': None}\n","    state['model_path'] = model_path\n","    state['labels_path'] = labels_path\n","    # reset loaded model so it reloads with new paths\n","    state['model'] = None\n","    state['labels'] = None\n","    return state, f\"Live model set:\\n{model_path}\\n{labels_path}\"\n","\n","# ==================== Gradio UI ====================\n","with gr.Blocks(title=\"Action Detection — Collect / Train / Test (Real-time)\") as demo:\n","    gr.Markdown(f\"**Base directory:** `{BASE_DIR}`  \\nKeypoints → `{DATA_PATH}`  \\nModels → `{MODELS_DIR}`\")\n","\n","    # Tab 1: Collect (now with landmark preview)\n","    with gr.Tab(\"1) Collect\"):\n","        with gr.Row():\n","            action_name_tb = gr.Textbox(label=\"Action name\", placeholder=\"e.g., wave, hello, thanks\")\n","            target_count = gr.Number(label=\"Target sequences\", value=30, precision=0)\n","            create_btn = gr.Button(\"Create / Select Folder\")\n","        status1 = gr.Markdown(\"\")\n","        progress = gr.Textbox(label=\"Progress\", value=\"0 / 30\", interactive=False)\n","\n","        gr.Markdown(\"**Record a short clip (5–10s) for ONE sequence**. We will save keypoints and also render an annotated preview with landmarks.\")\n","        with gr.Row():\n","            sequence_length = gr.Slider(10, 60, value=30, step=1, label=\"Frames per sequence\")\n","            stride = gr.Slider(1, 5, value=1, step=1, label=\"Stride (use every Nth frame)\")\n","        video_in = gr.Video(label=\"Video (webcam or upload)\", sources=[\"webcam\", \"upload\"], height=300)\n","        save_btn = gr.Button(\"Record & Save (one sequence)\")\n","\n","        # >>> NEW: annotated preview outputs\n","        anno_path = gr.Textbox(label=\"Annotated preview file (.mp4)\", interactive=False)\n","        anno_frame = gr.Image(label=\"Preview (first annotated frame)\", visible=True)\n","\n","        really = gr.Checkbox(label=\"Really reset this action's data?\")\n","        reset_btn = gr.Button(\"Reset Action Folder\", variant=\"stop\")\n","\n","        create_btn.click(create_action_folder, [action_name_tb, target_count], [status1, action_name_tb, progress])\n","        save_btn.click(\n","            record_and_save,\n","            [action_name_tb, video_in, sequence_length, stride, target_count],\n","            [status1, progress, anno_path, anno_frame]\n","        )\n","        reset_btn.click(reset_action, [action_name_tb, really], [status1, action_name_tb])\n","\n","    # Tab 2: Train / Evaluate\n","    with gr.Tab(\"2) Train / Evaluate\"):\n","        with gr.Row():\n","            seq_len_train = gr.Slider(10, 60, value=30, step=1, label=\"Sequence length\")\n","            test_size = gr.Slider(0.1, 0.5, value=0.2, step=0.05, label=\"Test size\")\n","        with gr.Row():\n","            epochs = gr.Slider(1, 50, value=12, step=1, label=\"Epochs\")\n","            batch = gr.Slider(4, 64, value=16, step=4, label=\"Batch size\")\n","        train_btn = gr.Button(\"Train LSTM\")\n","        train_msg = gr.Textbox(label=\"Summary\", lines=4)\n","        loss_img = gr.Image(label=\"Training Loss\")\n","        cm_img = gr.Image(label=\"Confusion Matrix\")\n","        report_txt = gr.Textbox(label=\"Classification Report\", lines=14)\n","        train_btn.click(train_model, [seq_len_train, test_size, epochs, batch], [train_msg, loss_img, cm_img, report_txt])\n","\n","    # Tab 3a: Batch Test (existing)\n","    with gr.Tab(\"3a) Test (clip based)\"):\n","        gr.Markdown(\"Test with a short **webcam/uploaded clip** or an existing **dataset sequence**.\")\n","        with gr.Row():\n","            with gr.Column():\n","                gr.Markdown(\"**Webcam/Upload Clip Test**\")\n","                model_path = gr.Textbox(label=\"Model .keras path\")\n","                labels_path = gr.Textbox(label=\"Labels .json path\")\n","                seq_len_inf = gr.Slider(10, 60, value=30, step=1, label=\"Sequence length\")\n","                stride_inf = gr.Slider(1, 5, value=1, step=1, label=\"Stride\")\n","                vid_inf = gr.Video(label=\"Video (webcam or upload)\", sources=[\"webcam\", \"upload\"], height=300)\n","                infer_btn = gr.Button(\"Predict (Clip)\")\n","                preds_json = gr.Textbox(label=\"Top-k predictions (JSON)\", lines=8)\n","                preds_plot = gr.Image(label=\"Scores\")\n","                infer_btn.click(predict_from_webcam, [model_path, labels_path, vid_inf, seq_len_inf, stride_inf], [preds_json, preds_plot])\n","\n","            with gr.Column():\n","                gr.Markdown(\"**Dataset Test**\")\n","                action_sel = gr.Dropdown(label=\"Action\", choices=list_actions())\n","                seq_idx = gr.Number(label=\"Sequence #\", value=1, precision=0)\n","                seq_len_ds = gr.Slider(10, 60, value=30, step=1, label=\"Sequence length\")\n","                infer_ds_btn = gr.Button(\"Predict (Dataset)\")\n","                preds_json_ds = gr.Textbox(label=\"Top-k predictions (JSON)\", lines=8)\n","                preds_plot_ds = gr.Image(label=\"Scores\")\n","                infer_ds_btn.click(predict_from_dataset, [model_path, labels_path, action_sel, seq_idx, seq_len_ds], [preds_json_ds, preds_plot_ds])\n","\n","    # >>> NEW Tab 3b: True Real-time (streaming)\n","    with gr.Tab(\"3b) Live Test (real-time)\"):\n","        gr.Markdown(\"Live, frame-by-frame predictions with a sliding window and on-frame landmarks & scores.\")\n","        live_state = gr.State({'deque': collections.deque(maxlen=30), 'model': None, 'labels': None})\n","\n","        with gr.Row():\n","            live_model_path = gr.Textbox(label=\"Model .keras path (for live)\")\n","            live_labels_path = gr.Textbox(label=\"Labels .json path (for live)\")\n","            seq_len_live = gr.Slider(10, 60, value=30, step=1, label=\"Sequence length (window)\")\n","            topk_live = gr.Slider(1, 5, value=3, step=1, label=\"Top-K to display\")\n","\n","        set_live_btn = gr.Button(\"Use these paths for LIVE\")\n","        set_msg = gr.Textbox(label=\"Status\", interactive=False)\n","        set_live_btn.click(set_live_model_paths, [live_state, live_model_path, live_labels_path], [live_state, set_msg])\n","\n","        # Attempting gr.Video with sources=[\"webcam\"] instead of gr.Image(live=True)\n","        cam_live = gr.Video(label=\"Webcam (live)\", sources=[\"webcam\"], height=360)\n","        out_live = gr.Image(label=\"Annotated (live)\")\n","        json_live = gr.Textbox(label=\"Top-K (JSON)\", lines=6)\n","\n","        # Stream frames from gr.Video → model\n","        # cam_live.stream(  # This caused AttributeError\n","        #     fn=realtime_stream,\n","        #     inputs=[cam_live, live_state, seq_len_live, topk_live],\n","        #     outputs=[out_live, json_live, live_state],\n","        #     time_limit=None\n","        # )\n","\n","        # Since stream() is not available, we'll disable the live tab for now or need a different approach\n","        gr.Markdown(\"Real-time streaming is not supported with the current Gradio version/setup.\")\n","\n","\n","demo.launch(share=True, server_name=\"0.0.0.0\", show_error=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":611},"id":"LEa2JmC1eU4h","executionInfo":{"status":"ok","timestamp":1761752447491,"user_tz":-570,"elapsed":1075,"user":{"displayName":"Avery Doan (AveryD)","userId":"04901609193817367419"}},"outputId":"d56a8be3-b435-4886-8fdc-0b9bf257c2eb"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://5265add320c3384bdb.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://5265add320c3384bdb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":22}]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyM7ra7A+PPMIWnP4Iprqk6d"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}